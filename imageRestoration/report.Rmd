---
title: "Accelerated Proximal Gradient for Image Restoration"
author: "Huijuan Zhou"
date: "Nov.7,2017"
output:
  html_document:
    mathjax: local
    self_contained: false
---

###Original image: Lan Somerhalder

<center>
![](./origImg.png)

###Damaged image

<center>
![](./damagedImage.png)

###Reconstructed image
<center>
![](./reconsImg.png)

----------

##Theory

- For martrix $A\in R^{m\times n}$, the condensed SVD is $A=U_r\Sigma_rV_r$. For the constant $\lambda>0$, the optimal solution of the optimization problem$$\mathrm{min}_{X\in R^{m\times n}}f(X)=\frac{1}{2}||X-A||^2_F+\lambda||X||_{S_1}$$
is$$\hat X=\mathcal{D}_{\lambda}(A):=U_r(\Sigma_r-\lambda I_r)_{+}V^T_r,$$

where$(\Sigma_r-\lambda I_r)_{+}=\mathrm{diag}((\sigma_1-\lambda)_{+},\cdots,(\sigma_r-\lambda)_{+})$.

$\hat X$ is the **singular value thresholding operator** of $X$.

--------

- The elements of $M\in R^{m\times n}$ is partially missing, and
<center>
$M_{ij}$ is obversed if and only is $(i,j)\in\Omega$

The **optimization problem for image reconstruction** is$$\mathrm{min}_{X\in R^{m\times n}}f(X)=\frac{1}{2}||P_{\Omega}(X-M)||^2_F+\lambda ||X||_{S_1}$$

Define $P_{\Omega}(Y)=\overset{\sim}{Y}=(\overset{\sim} y_{ij})\in R^{m\times n}$, and$$\overset{\sim} y_{ij}=\begin{cases} y_{ij}, &(i,j)\in\Omega\\0, &(i,j)\notin\Omega\end{cases}$$

-----------

- Let $f(X)=g(X)+h(X)$, where $g(X)=\frac{1}{2}||P_{\Omega}(X-M)||^2_F$.$$\nabla g(X)=P_{\Omega}(X-M),\nabla^2 g(X)\preceq K$$

where $K$ can equal $1$. So $g(X)$ can be controlled by$$\overset{\sim}g(X)=g(Y)+<\nabla g(Y),X-Y>+\frac{K}{2}||X-Y||^2_F$$
which equals$$\overset{\sim} g(X)=C+\frac{K}{2}||X-\{Y-\frac{1}{K}P_{\Omega}(Y-M)\}||^2_F$$

Replace $g(X)$ with $\overset{\sim} g(X)$, we get the **optimization problem**$$\mathrm{min}_{X\in R^{m\times n}}\frac{1}{2}||X-\{Y-\frac{1}{K}P_{\Omega}(Y-M)\}||^2_F+\frac{\lambda}{K} ||X||_{S_1}$$
**for each iteration**.

--------

##Algorithm

- Initialize $Y_1=M$, $X_1=M$, $X_0\in R^{m\times n}$, and $Y_0\in R^{m\times n}$, where $M\in R^{m\times n}$ is partially missing and known;

- Loop:

$\quad\quad X_0=X_1$ (store the last result)

$\quad\quad X_1=\mathcal{D}_{\frac{\lambda}{K}}(Y_1)$ (proximal operator)

$\quad\quad Y_0=X_1+\beta(X_1-X_0)$ (Accelerate)

$\quad\quad Y_1=Y_0+\frac{1}{K}P_{\Omega}(M-Y_0)$ 

- Until 

$\quad\quad ||X_1-X_0||_F<\epsilon$

------------

##Acceleration

parameter setting: $\lambda=1$, $K=1$, and

- $\beta=0$

![](./notacc.png)

- $\beta=0.2$

![](./acc0.2.png)

- $\beta=0.5$

![](./acc0.5.png)

- $\beta=0.8$

![](./acc0.8.png)